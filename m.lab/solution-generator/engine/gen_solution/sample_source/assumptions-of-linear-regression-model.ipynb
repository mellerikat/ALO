{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <p style=\"padding:10px;background-color:#85BB65;margin:0;color:white;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;overflow:hidden;font-weight:500\">Assumptions Of Linear Regression</p>\n\nWhile Building our Linear Regression Model we have some assumptions which we need to keep in our mind to better regression line fit for our Model.\n\nLinear Regression is supervised machine Learning Algorithm in which one or more independent variable explain the dependent(Predictor) variable. There linear regression have five assumptions.\n\n\n#### **<mark style=\"background-color:#85BB65;color:white;border-radius:5px;opacity:1.0\">assumptions of linear regression</mark>**\n\n\n* 1- Linearity\n* 2- Multicollinearity\n* 3- mean of residuals\n* 4- normality of residuals\n* 5- Error Term should be independent to each other\n* 6- hemoscedasticity / heteroscedasticity\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error ,r2_score ,mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(r\"/kaggle/input/advertising-dataset/Advertising.csv\").set_index(\"Unnamed: 0\")\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def two_plots_num_column(feature):\n    \n    print(f\"the skewness value of {feature} column = {df[feature].skew():.2f}\")\n    plt.figure(figsize=(10,4))\n    \n    plt.subplot(1,2,1)\n    plt.title('histgram')\n    sns.histplot(data=df, x=feature, kde=True)\n    plt.axvline(x = df[feature].mean(), c = 'red')\n    plt.axvline(x = df[feature].median(), c = 'green')\n\n    plt.subplot(1,2,2)\n    plt.title('Boxplot')\n    sns.boxplot(y=df[feature])\n    plt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"two_plots_num_column(\"Newspaper\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* `Newspaper`column is Right-skewed.","metadata":{}},{"cell_type":"code","source":"q1, q3 = df['Newspaper'].quantile([0.25, 0.75])\niqr = q3 - q1\nlower_bound = q1 - (1.5 * iqr)\nupper_bound = q3 + (1.5 * iqr)\n\ndf.loc[(df[\"Newspaper\"] < lower_bound) | (df[\"Newspaper\"] > upper_bound), \"Newspaper\"] = np.nan\ndf[\"Newspaper\"].fillna(df[\"Newspaper\"].mean(), inplace=True)\n            ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"two_plots_num_column('Sales')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>I <span style='color:#85BB65'>|</span> Linearity</b> \n\n<br> \n\n#### **<mark style=\"background-color:#85BB65;color:white;border-radius:5px;opacity:1.0\">Note that</mark>**\n\n* The relationship between X and the mean of Y is linear. If not linear, we may use polynomial regression or machine-learning techniques.\n\n* Linear regression needs the relationship between the independent and dependent variables to be linear. \n\n<br>\n\n<div style=\"border-radius:10px;border:#85BB65 solid;padding: 15px;background-color:#ffffff00;font-size:100%;text-align:left\">\nIf the independent and dependent variable are not linearly dependent on each other and we still try to fit the straight line. it will not give the better accuracy in the model.\n</div>\n\nLet's use a pair plot to check the relation of independent variables with the Sales variable","metadata":{}},{"cell_type":"code","source":"sns.pairplot(df, x_vars=['TV','Radio','Newspaper'], y_vars='Sales', size=5, aspect=0.7);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#85BB65'>|</span> Observations </b> \n\n\n* `TV` feature sean to have linear relationship with sales.\n* `Radio` feature doesn't form an accurately linear shape with the `Sales` variable but Radio do still better than `Newspaper` which seems to hardly have any specific shape.\n\nSo it shows that a linear regression fitting might not be the best model for it.","metadata":{}},{"cell_type":"markdown","source":"# <b>II <span style='color:#85BB65'>|</span> Multicollinearity</b> \n\nIt means that the independant variables should not have any correlation between each other. \n\nTo check this issue we can plot the pairwise correlation plot and avoid using high correlated variables","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nsns.pairplot(df)\nplt.show();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Sometimes two or more variables are correlated to a independant variable which is hard to identify from correlation plot. in that case you can check the VIF (Variance Inflation Factor).\n\n### **<span style='color:#85BB65'>What's the Variance Inflation Factor (VIF)? </span>** \n\nVIF value ranges between 1 to infinity . value 1 indecation no multicollinearity and the higher value of VIF , the higher value of multicollinearity.\n\n* VIF between 1:5 indecating moderate multicollinearity.\n* VIF between 5:10 indecating higher level of multicollinearity.\n* VIF between 10:.. indecating very high level multicollinearity.","metadata":{}},{"cell_type":"code","source":"# check about multicollenarity\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\ncolumns= df.drop(columns='Sales').columns\n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = columns\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(df.drop(columns='Sales').values, i)\n                          for i in range(len(columns))]\n  \nvif_data","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#85BB65'>|</span> Observations </b> \n\n* all feature have value of VIF Less than 5. that is very suitable","metadata":{}},{"cell_type":"markdown","source":"now let's build the model and check about other assumptions.","metadata":{}},{"cell_type":"code","source":"X = df.drop([\"Sales\"],axis=1)\ny = df.Sales","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 42 ,test_size=0.25)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build an fit the model \n\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)\n\ny_pred= model.predict(X_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"R squared: {}\".format(r2_score(y_true=y_train ,y_pred=y_pred)))\nprint(f\"mae : {mean_absolute_error(y_train,y_pred)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>III <span style='color:#85BB65'>|</span> Mean of residuals</b> \n\nmean of residuals should be equal zero.","metadata":{}},{"cell_type":"code","source":"# create a list of residuals \nresiduals = y_train.values - y_pred\n\nmean_residuals = np.mean(residuals)\nprint(\"Mean of Residuals {}\".format(mean_residuals))","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#85BB65'>|</span> Observations </b> \n\n* The mean of residuals is almost equal to zero. That's very good","metadata":{}},{"cell_type":"markdown","source":"# <b>IV <span style='color:#85BB65'>|</span> Normality of residuals</b> \n\nit is assume that the error term is normally distributed","metadata":{}},{"cell_type":"code","source":"# Plot the histogram of the error terms\n\nfig = plt.figure()\nsns.distplot(residuals , bins=20)\nfig.suptitle('Error Terms', fontsize = 20)    \nplt.xlabel('Errors', fontsize = 18)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#85BB65'>|</span> Observations </b> \n\n* Error terms is approximately lift-skew distributed. it means that a linear regression fitting didn't be the best.","metadata":{}},{"cell_type":"markdown","source":"# <b>V <span style='color:#85BB65'>|</span> Error Term should be independent to each other</b> \n\nit means that the error term should not dependent in any other error terms. \nThe below diagram shows that the error term are randomly distributed and not following any pattern.","metadata":{}},{"cell_type":"code","source":"plt.scatter(y_pred , residuals)\nplt.axhline(y=0,color=\"red\" ,linestyle=\"--\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <b>VI <span style='color:#85BB65'>|</span> Hemoscedasticity And Heteroscedasticity</b>\n\n* hemoscedasticity means that variance should not be increasing or decreasing (constant) if error term changes (increase or decrease). Also it should not be following some pattern as error term changes (increase or decrease). \n\n* Heteroskedasticity refers to the situation where the variance of the residuals in a regression model is not constant across different levels of the predictor variables.\n\n![](https://media.geeksforgeeks.org/wp-content/uploads/20190425172205/hetero.jpg)\n\n<br>\n\n<div style=\"border-radius:10px;border:#85BB65 solid;padding: 15px;background-color:#ffffff00;font-size:100%;text-align:left\">\nThe null hypothesis of the test is that the variance of the residuals is constant (homoscedastic), while the alternative hypothesis is that the variance of the residuals is not constant (heteroskedastic).\n</div>\n\n<br>\n\nto dentify heterscedasticity , we will use statistical test called `breusch-pagan`.\nthis test check whether heterscedasticity exists or not.\n","metadata":{}},{"cell_type":"code","source":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nname = ['f_statistic' , 'p_value' , 'lagrange multipler stat']\ntest = sms.het_breuschpagan(residuals , X_train)\nlzip(name , test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hope you liked the notebook, any suggestions would be highly appreciated.","metadata":{}},{"cell_type":"markdown","source":"***\n\n<br>\n\n<div style=\"text-align: center;\">\n   <span style=\"font-size: 4.5em; font-weight: bold; font-family: Arial;\">THANK YOU!</span>\n</div>/\n\n<br>\n<br>\n\n<div style=\"text-align: center;\">\n    <span style=\"font-size: 5em;\">✔️</span>\n</div>\n\n<br>\n\n<div style=\"text-align: center;\">\n   <span style=\"font-size: 1.4em; font-weight: bold; font-family: Arial; max-width:1200px; display: inline-block;\">\n       If you find this notebook useful, I'd greatly appreciate your upvote!\n   </span>\n</div>\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}