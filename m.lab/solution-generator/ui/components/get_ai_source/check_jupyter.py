import os
import shutil
import nbformat
import re
import pandas as pd
import altair as alt
import streamlit as st
import tiktoken
import zipfile
from datetime import datetime
from engine.chatgpt import chatgpt_query
from ui.src.chat_prompts.read_prompt import read_prompt

CONDITION_POINT = 7
TOKEN_MARGIN = 500 # for langgraph alo template 

class CheckJupyter:
    """MLOps ì‹¤í–‰ ì í•©ì„±ì„ 6ê°€ì§€ í•­ëª©ì— ëŒ€í•´ í‰ê°€"""
    def __init__(self, api, path):
        self.api = api
        self.download_path = path['source_notebook']
        self.py_path = path['source_py']
        self.metadata_path = path['metadata']
        # check í•˜ë ¤ëŠ” ë…¸íŠ¸ë¶ì´ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
        self.update_file = None    
        # download_pathì— ì¡´ì¬í•˜ëŠ” íŒŒì¼, self.notebookê³¼ êµ¬ë¶„ë˜ì–´ì•¼ í•¨
        st.session_state['download_file'] = None    
        st.session_state['analysis_result'] = None

    def click_check_btn(self, notebook):
        """get_ai_sourceì—ì„œ ë²„íŠ¼ì´ í´ë¦­ë˜ë©´ í•¨ìˆ˜ í˜¸ì¶œ"""
        self.update_file = notebook

    def extract_notebook_content(self, ipynb_file_path, only_code=False):
        """Jupyter ë…¸íŠ¸ë¶ íŒŒì¼ì˜ ë‚´ìš©ì„ ì¶”ì¶œ"""
        with open(ipynb_file_path, 'r', encoding='utf-8') as f:
            notebook = nbformat.read(f, as_version=4)

        content = ''
        for cell in notebook.cells:
            if cell.cell_type == 'code':
                content += cell.source + '\n\n'
        return content

    def analyze_mlops_compatibility(self, notebook_content):
        """ChatGPTë¥¼ ì‚¬ìš©í•œ MLOps ì í•©ì„± ë¶„ì„ ë° ì„¤ëª…"""
        template = read_prompt('analysis_mlops_score.md')
        print(template)
        prompt = template.format(notebook_content=notebook_content)

        try:
            result = chatgpt_query(prompt)
            parts = result.strip().split('==Divide==')
            response1 = parts[0].strip()  # Divide ì´ì „ ë¶€ë¶„
            response2 = parts[1].strip()  # Divide ì´í›„ ë¶€ë¶„
            # FIXME 
            response2 = response2.replace('Request2', 'Metadata extracted from code')
            response2 = response2.replace('Request 2', 'Metadata extracted from code')
            # metadata_pathì— ì €ì¥ë˜ë„ë¡ ì„¤ì • (ì—†ìœ¼ë©´ ìƒì„±)
            metadata_file_path = os.path.join(self.metadata_path, "metadata.md")
            # í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
            os.makedirs(self.metadata_path, exist_ok=True)
            # response2ë¥¼ metadata.md íŒŒì¼ë¡œ ì €ì¥
            with open(metadata_file_path, "w") as file:
                file.write(response2)
            print(f"metadata for data ë‚´ìš©ì´ {metadata_file_path} íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            scores, reasons = self.parse_analysis_scores_and_reasons(response1)
            return {
                'title': st.session_state['download_file'], 
                'scores': scores,
                'reasons': reasons
            }
        except Exception as e:
            return {
                'title': None, 
                'scores': {},
                'reasons': f"ë…¸íŠ¸ë¶ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}"
            }

    def parse_analysis_scores_and_reasons(self, response):
        """ChatGPT ì‘ë‹µì—ì„œ ì ìˆ˜ ë° í‰ê°€ ì´ìœ  ì¶”ì¶œ"""
        scores = {
            'Training Code': 0,
            'Inference Code': 0,
            'Model Evaluation': 0,
            'Preprocess Code': 0,
            'Input Definition': 0,
            'Output Definition': 0
        }
        reasons = {
            'Training Code': '',
            'Inference Code': '',
            'Model Evaluation': '',
            'Preprocess Code': '',
            'Input Definition': '',
            'Output Definition': ''
        }
        matches = re.findall(r'\d+\.\s+.*?\?\s*(\d+)P\s*(.*?)(?=\d+\.|$)', response, re.DOTALL)
        for i, match in enumerate(matches, 1):
            score, reason = match
            components = {'1': 'Training Code', '2': 'Inference Code', '3': 'Model Evaluation', '4':'Preprocess Code', '5':'Input Definition', '6': 'Output Definition'}
            scores[components[str(i)]] = int(score.strip())
            reasons[components[str(i)]] = reason.strip() 
        return scores, reasons

    def display_analysis(self):
        """ë¶„ì„ ê²°ê³¼ ë° ì´ìœ  UIì— í‘œì‹œ"""
        selected_file = self.update_file
        analysis_result = st.session_state['analysis_result']
        scores = analysis_result['scores']
        reasons = analysis_result['reasons']
        if not scores:
            st.write("No scores to display.")
            return
        df_scores = pd.DataFrame(list(scores.items()), columns=['í•­ëª©', 'ì ìˆ˜'])
        df_scores['ì´ìœ '] = df_scores['í•­ëª©'].map(reasons)
        with st.expander("ğŸ“Š Evaluation Criteria Explained ğŸ“"):
            st.markdown("""
            MLOps ì‹¤í–‰ ì í•©ì„±ì„ 6ê°€ì§€ í•­ëª©ì— ëŒ€í•´ í‰ê°€í•©ë‹ˆë‹¤. ê° ê¸°ì¤€ì€ ìµœëŒ€ 10ì  ë§Œì ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤. ì„±ê³µì ì¸ í‰ê°€ë¥¼ ìœ„í•´ì„œëŠ” 1ë²ˆì—ì„œ 3ë²ˆ í•­ëª©ì´ ë¬´ì¡°ê±´ 7ì  ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤. ê¸°ì¤€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

            1. **Training Code ğŸ‹ï¸**: <span style="color:red">ëª¨ë¸ í›ˆë ¨ ì½”ë“œë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆê¹Œ?</span>
            2. **Inference Code ğŸ“ˆ**: <span style="color:red">ì…ë ¥ ë°ì´í„°ì— ëŒ€í•œ ì¶”ë¡  ì½”ë“œë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆê¹Œ?</span>
            3. **Model Evaluation ğŸ“Š**: <span style="color:red">ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ì§€í‘œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆê¹Œ?</span>
            4. **Data Preprocessing ğŸ§¹**: ë°ì´í„° ì „ì²˜ë¦¬ ì½”ë“œë¥¼ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆê¹Œ?
            5. **Input Definition ğŸ“‚**: í›ˆë ¨ ë° ì¶”ë¡  ë°ì´í„°ì˜ ì •ì˜ê°€ ëª…í™•í•©ë‹ˆê¹Œ?
            6. **Output Definition ğŸ¯**: ì¶”ë¡  í›„ ìƒì„±ë  ì¶œë ¥ì´ ëª…í™•í•©ë‹ˆê¹Œ?
            
            ì°¸ê³ ë¡œ, python íŒŒì¼ì— ë“¤ì–´ê°€ëŠ” ë‚´ìš©ì€ í•™ìŠµ í˜¸ì¶œ í•¨ìˆ˜ì™€ ì¶”ë¡  í˜¸ì¶œ í•¨ìˆ˜ë¥¼ ë¶„ë¦¬í•´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”. 
            ì˜ˆë¥¼ë“¤ì–´, train()ê³¼ inference() í•¨ìˆ˜ê°€ ë³„ë„ë¡œ í˜¸ì¶œë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë‘ í•¨ìˆ˜ë¥¼ í•˜ë‚˜ì˜ ë³„ë„ í•¨ìˆ˜ì— í•¨ê»˜ ë„£ì§€ ë§ì•„ì£¼ì„¸ìš”.  
            """, unsafe_allow_html=True)
        chart = alt.Chart(df_scores).mark_bar(color='skyblue').encode(
            x=alt.X('í•­ëª©:N', sort=None),
            y=alt.Y('ì ìˆ˜:Q', scale=alt.Scale(domain=[0, 10])),
            tooltip=['í•­ëª©', 'ì ìˆ˜', 'ì´ìœ ']
        )
        rule = alt.Chart(df_scores).mark_rule(
            color='red'
        ).encode(
            y=alt.datum(7)
        )
        combined_chart = alt.layer(chart, rule).configure_axis(
            domain=False,
            grid=True
        ).properties(
            width=700,
            height=400
        )
        st.altair_chart(combined_chart)
        # ì„±ê³µ ì¡°ê±´ ê²€ì‚¬
        successful_criteria = (df_scores['í•­ëª©'] == 'Training Code') & (df_scores['ì ìˆ˜'] >= CONDITION_POINT) | \
                              (df_scores['í•­ëª©'] == 'Inference Code') & (df_scores['ì ìˆ˜'] >= CONDITION_POINT) | \
                              (df_scores['í•­ëª©'] == 'Model Evaluation') & (df_scores['ì ìˆ˜'] >= CONDITION_POINT)
        if successful_criteria.sum() == 3:
            st.success("ğŸ‰ Notebook has passed the MLOps compatibility check! ğŸ¥³")
            return True # passí–ˆëŠ”ì§€ return
        else:
            st.warning("ğŸš¨ Notebook did not pass MLOps compatibility check. It has been deleted due to failing the score threshold.")
            file_path = os.path.join(self.download_path, f"{selected_file}.ipynb")
            py_path = os.path.join(self.download_path, f"{selected_file}.py")
            file_path2 = os.path.join(self.metadata_path, f"metadata.md")
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ëª©ë¡ì„ ë°˜ì˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‚­ì œí•˜ê³ , ë‹¤ì‹œ ì„ íƒë˜ì§€ ì•Šë„ë¡ ì„¤ì •
                    st.session_state['download_file'] = None
                if os.path.exists(py_path):
                    os.remove(py_path)
                    # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ëª©ë¡ì„ ë°˜ì˜í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ ì‚­ì œí•˜ê³ , ë‹¤ì‹œ ì„ íƒë˜ì§€ ì•Šë„ë¡ ì„¤ì •
                    st.session_state['download_file'] = None

                if os.path.exists(file_path2):
                    os.remove(file_path2)
                st.success(f"ğŸ—‘ï¸ '{selected_file}' and its corresponding file have been deleted.")
            except Exception as e:
                st.error(f"âŒ Failed to delete {selected_file}: {e}")
            return False

    def check_jupyter(self):

        def get_max_out_token(model_id): 
            model_id_out_token_table = {
                            'gpt-4o-2024-08-06': 16384,
                            'gpt-4o-mini-2024-07-18': 16384,
                            'gpt-4o-2024-05-13': 4096,
                            'gpt-4-turbo-2024-04-09': 4096,
                            'gpt-4-0125-Preview': 4096,
                            'gpt-4-vision-preview': 4096,
                            'gpt-4-1106-Preview': 4096,
                            'gpt-35-turbo-0125': 4096,
                            'gpt-35-turbo-1106': 4096
                            }
            if model_id in model_id_out_token_table: 
                max_out_token = model_id_out_token_table[model_id]
                print(f"[INFO] max out token: {max_out_token}")
                return max_out_token
            else: 
                print(f"[WARNING] Your OPENAI_MODEL_ID: {model_id} is not supported. Your output token is limited to 4096.")
                return 4096 

        self.max_token = get_max_out_token(os.getenv('OPENAI_MODEL_ID')) - TOKEN_MARGIN

        def count_tokens(text: str, model: str = 'cl100k_base') -> int:
            # GPT-4ì˜ í† í¬ë‚˜ì´ì € ì‚¬ìš©ì„ ê°€ì •
            enc = tiktoken.get_encoding(model)  # GPT-4 ëª¨ë¸ëª…ì— ë§ëŠ” ì¸ì½”ë”© ì§€ì •
            tokens = enc.encode(text)
            return len(tokens)
        title = st.session_state['download_file']
        if os.path.exists(self.py_path):
            shutil.rmtree(self.py_path)
        # re-create source_py path 
        os.makedirs(self.py_path)
        content, token_length = None, None
        is_single_file = True 
        
        if len(os.listdir(self.download_path)) != 1: 
            st.error("1ê°œì˜ íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.") 
        # ipynb case
        if os.path.exists(os.path.join(self.download_path, f"{title}.ipynb")):
            content = self.extract_notebook_content(os.path.join(self.download_path, f"{title}.ipynb"))
        # single py case
        elif os.path.exists(os.path.join(self.download_path, f"{title}.py")):
            py_path = os.path.join(self.download_path, f"{title}.py")
            with open(py_path, 'r', encoding='utf-8') as file:
                content = file.read()
        # zip case (src directory + main.py) - multi files
        elif os.path.exists(os.path.join(self.download_path, f"{title}.zip")):
            zip_file_path = os.path.join(self.download_path, f"{title}.zip")
            extract_zip_to_directory(zip_file_path, self.download_path)
            # check if only main.py and src directory exist 
            if check_zip_contents(self.download_path):
                main_py_path = os.path.join(self.download_path, "main.py") 
                header_content = """from pathlib import Path
 
# í˜„ì¬ íŒŒì¼(d.py)ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 'src' ê²½ë¡œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
src_path = Path(__file__).parent / 'src'
for root, dirs, files in os.walk(src_path):
    module_path = Path(root)
    if str(module_path) not in sys.path:
        sys.path.insert(0, str(module_path))
"""
                with open(main_py_path, 'r', encoding='utf-8') as file:
                    content = file.read()
                new_content = header_content + content
                with open(main_py_path, 'w', encoding='utf-8') as file:
                    file.write(new_content)

                is_single_file = False # multi files
            else:
                st.error("ì••ì¶• íŒŒì¼ ë‚´ì— 'main.py' íŒŒì¼ê³¼ 'src' í´ë”ë§Œ í¬í•¨ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.") 
        else:
            st.error("ë‹¤ìš´ë¡œë“œëœ ì†ŒìŠ¤ ì½”ë“œê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ìš”êµ¬ë˜ëŠ” íŒŒì¼êµ¬ì¡°ì— ë¶€í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ê°€ì´ë“œëŒ€ë¡œ ì†ŒìŠ¤ ì½”ë“œë¥¼ ë‹¤ìš´ë¡œë“œ í˜¹ì€ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        if content:
            token_length = count_tokens(content)
            if token_length > self.max_token:
                st.error(f"í† í°ì˜ ê°œìˆ˜ê°€ {token_length} ì…ë‹ˆë‹¤, í˜„ì¬ {self.max_token} í† í°ê¹Œì§€ë§Œ ì§€ì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")
            else: 
                st.session_state['analysis_result'] = self.analyze_mlops_compatibility(content)
                is_passed = self.display_analysis() # ì ìˆ˜ í†µê³¼ ì—¬ë¶€ 

                header_content = """from pathlib import Path
 
import sys
# í˜„ì¬ íŒŒì¼(d.py)ì˜ ë¶€ëª¨ ë””ë ‰í† ë¦¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 'src' ê²½ë¡œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
src_path = Path(__file__).parent / 'src'
for root, dirs, files in os.walk(src_path):
    module_path = Path(root)
    if str(module_path) not in sys.path:
        sys.path.insert(0, str(module_path))
"""
                new_content = header_content + content
                if is_passed and is_single_file:
                    script_path = os.path.join(self.py_path, f"{title}.py")
                    with open(script_path, 'w', encoding='utf-8') as f:
                        f.write(new_content)
                elif is_passed and not is_single_file:
                    script_path = os.path.join(self.py_path, f"main.py")
                    with open(script_path, 'w', encoding='utf-8') as f:
                        f.write(new_content)
                    # copy source_notebook/src --> source_py/src
                    shutil.copytree(os.path.join(self.download_path, 'src'), os.path.join(self.py_path, 'src'))
                                    
    def render(self):
        st.divider()
        st.subheader("Pre-condition: Check Code Quality")
        # íŒŒì¼ì€ ìœ ì§€ë˜ë‚˜ í˜ì´ì§€ê°€ ìƒˆë¡œ ë Œë”ë§ ë˜ëŠ” ê²½ìš° displayë§Œ í˜¸ì¶œ
        analysis_result = st.session_state['analysis_result']
        if analysis_result and analysis_result['title'] == self.update_file:
            st.write(f"Checking notebook: {self.update_file}")
            with st.spinner('Wait for checking...'):
                self.display_analysis()
        # íŒŒì¼ ë‹¤ìš´ë¡œë“œ(ë³€ê²½) ì‹œ check_jupyter ì‹¤í–‰
        elif self.update_file:
            print(self.update_file)
            st.session_state['download_file'] = self.update_file
            st.write(f"Checking notebook: {self.update_file}")
            with st.spinner('Wait for checking...'):
                self.check_jupyter()
        else:
            st.error("ë‹¤ìš´ë¡œë“œëœ ì†ŒìŠ¤ ì½”ë“œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì†ŒìŠ¤ ì½”ë“œë¥¼ ë‹¤ìš´ë¡œë“œ í˜¹ì€ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")

def extract_zip_to_directory(zip_path, extract_to):
    if not os.path.exists(zip_path):
        print(f".zip íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {zip_path}")
        return
    if not os.path.isdir(extract_to):
        print(f"ì¶”ì¶œí•  ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {extract_to}")
        os.makedirs(extract_to)
        print(f"ë””ë ‰í† ë¦¬ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤: {extract_to}")
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_to)
    print(f"{zip_path} íŒŒì¼ì˜ ëª¨ë“  ë‚´ìš©ì„ {extract_to} ë””ë ‰í† ë¦¬ì— ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.")
    os.remove(zip_path)
    print(f"{zip_path} íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤.")
    
def check_zip_contents(extracted_zip_path):
    items = os.listdir(extracted_zip_path)
    required_items = {'main.py', 'src'}
    if set(items) == required_items:
        print("[SUCCESS] ì••ì¶• íŒŒì¼ ë‚´ì— 'main.py' íŒŒì¼ê³¼ 'src' í´ë”ë§Œ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        return True
    else:
        print("f[FAIL] ì••ì¶• íŒŒì¼ ë‚´ì— 'main.py' íŒŒì¼ê³¼ 'src' í´ë”ë§Œ í¬í•¨ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬ êµ¬ì„±ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: \n {set(items)}")
        return False