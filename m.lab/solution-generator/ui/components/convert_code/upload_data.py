import os
import re 
import shutil
import subprocess 
import json
from uuid import uuid4
import zipfile
from git import Repo
import streamlit as st
from engine.chatgpt import chatgpt_query
from ui.components.get_ai_source.recommend_jupyter import RecommendJupyter

from path_list import *
            
class KaggleDataRecommender(RecommendJupyter): 
    def __init__(self, api, metadata_path):
        super().__init__(api, None, None)  
        self.api = api
        self.metadata_file_path = os.path.join(metadata_path, "metadata.md")
        self.data_size_limit = 50 * 1024 * 1024 # under 50MB 
        
    def read_metadata(self): 
        metadata = None
        if os.path.exists(self.metadata_file_path):
            with open(self.metadata_file_path, "r") as file:
                metadata = file.read()
        else: 
            st.warning("Warning: metadata.md for source code is not generated yet.")
        return metadata 

    def recommend_dataset_topic(self):
        llm_metadata_md = self.read_metadata()

        recommend_dataset_topic_promt = f"""아래 파일을 읽고 이 데이터 셋의 주요 키워드를 두개 뽑아줘.
        이 데이터 셋을 kaggle api를 통해 탐색할 수 있는 데이터 이름 기반의 키워드로 추천해줘.
        ```
        {llm_metadata_md}
        ```
        다른 내용 없이 정확히 키워드를 반드시 영어로 두 개만 응답해줘.
        """

        try:
            response_topic = chatgpt_query(recommend_dataset_topic_promt)
        except Exception as e:
            st.error(f"ChatGPT API 호출에 실패했습니다: {e}")
            return None
        return response_topic

    def recommend_datasets(self, datasets_metadata):
        """# LLM을 통해 관련된 데이터셋 추천"""
        datasets_metadata_text = json.dumps(datasets_metadata, indent=4)
        # metadata generated by llm from source code 
        llm_metadata_md = self.read_metadata()
        # TODO: 프롬프트 분리 필요
        recommendations_input = f"""
        The user has provided the following information. If the information is not in English, please translate it into English first:
        ```
        {llm_metadata_md}
        ```
        Based on the provided information, recommend 5 datasets from the following list with explanation: If the description section lacks an explanation of the data column names, please do not select it.
        ```
        {datasets_metadata_text}
        ```
        Please provide the recommendations in Korean.

        Output should be in the following format:
        1. **Dataset Name**: https://www.kaggle.com/datasets/example/dataset 
        - **Reason**: Explanation of why this dataset is suitable (relevance to the dataset, project suitability, etc.)
        """
        try:
            # print('recommendations_input: \n', recommendations_input)
            response = chatgpt_query(recommendations_input)
        except Exception as e:
            st.error(f"ChatGPT API 호출에 실패했습니다: {e}")
            return None
        # print(response)
        recommended_refs = re.findall(r'https://www.kaggle.com/datasets/([^ \n]+)', response)
        recommended_refs = [s.replace('(', '').replace(')', '') for s in recommended_refs] # remove ( ) 
        if len(recommended_refs) == 0: 
            recommended_refs = re.findall(r'https://www.kaggle.com/([^ \n]+)', response)
            recommended_refs = [s.replace('(', '').replace(')', '') for s in recommended_refs] 
        return response, recommended_refs

    def recommend_kaggle_data(self): 
        match_task = re.search(r'- \*\*Task\*\*:\s*([\w\s]+)', self.read_metadata())
        resp_topic = self.recommend_dataset_topic()
        task = [match_task.group(1).strip(), resp_topic]
        datasets = self.search_datasets(keyword=task, max_results=20, max_size=self.data_size_limit)
        #st.session_state['dataset_notebooks'] = {}
        # step2: 데이터셋 정보 kaggle에서 가져오기
        datasets_metadata = {}
        for dataset in datasets:
            ref = dataset.ref
            metadata = self.fetch_dataset_metadata(ref)
            if metadata is None:
                continue
            datasets_metadata[metadata["title"]] = metadata
        # step3: 사용자의 정보와 관련된 데이터셋 추천
        resp, rec_datasets = self.recommend_datasets(datasets_metadata)
        return resp, rec_datasets, datasets_metadata
    
class UploadData:
    def __init__(self, api, path):
        self.api = api # kaggle api 
        self.data_path = path['data']
        self.metadata_path = path['metadata']

        if 'convert_data_description' not in st.session_state:
            st.session_state['convert_data_description'] = ''

        if not os.path.exists(self.data_path):
            os.makedirs(self.data_path)
        self.train_data_path = os.path.join(self.data_path, 'train')
        if not os.path.exists(self.train_data_path):
            os.makedirs(self.train_data_path)
        self.inference_data_path = os.path.join(self.data_path, 'inference')
        if not os.path.exists(self.inference_data_path):
            os.makedirs(self.inference_data_path)

    def save_and_extract_zip(self, uploaded_file, extract_path):
        # 파일을 임시 경로에 저장
        temp_zip_path = os.path.join(self.data_path, uploaded_file.name)
        with open(temp_zip_path, 'wb') as f:
            f.write(uploaded_file.getbuffer())
        self.extract_zip(temp_zip_path, extract_path)


    def extract_zip(self, uploaded_path, extract_path):
        if os.path.isfile(uploaded_path) or os.path.islink(uploaded_path):
            try:
                # init extracted path 
                if os.path.exists(extract_path):
                    shutil.rmtree(extract_path)
                os.makedirs(extract_path)
                if uploaded_path.endswith(".zip"):
                    with zipfile.ZipFile(uploaded_path, 'r') as zip_ref:
                        zip_ref.extractall(extract_path)
                else:
                    # If tar.gz, you need tarfile module to extract
                    import tarfile
                    with tarfile.open(uploaded_path, 'r:gz') as tar_ref:
                        tar_ref.extractall(extract_path)
            except zipfile.BadZipFile:
                st.error("Error: The uploaded file is not a valid zip file.")
            except EOFError: 
                st.error("Error: EOF error for zip file.")
            except FileNotFoundError:
                st.error(f"Error: The file '{uploaded_path}' does not exist.")
            except PermissionError:
                st.error(f"Error: Permission denied for file '{uploaded_path}'.")
            except Exception as e:
                st.error(f"An unexpected error occurred: {e}")
            finally: 
                os.remove(uploaded_path)
                print(f"----- removed {uploaded_path}")
            
        
    def view_metadata(self):
        # metadata_path에 저장되도록 설정 (없으면 생성)
        metadata_file_path = os.path.join(self.metadata_path, "metadata.md")
        if os.path.exists(metadata_file_path):
            with open(metadata_file_path, "r") as file:
                metadata = file.read()
            with st.expander('Generated metadata from source code'):
                st.markdown(metadata)
                st.divider()
        else: 
            st.warning("Warning: metadata.md for source code is not generated yet.")

    def run(self):
        return self.meta_data_info
    
    def set_input(self):
        st.subheader("Upload Train & Inference Data")
        st.caption('적용하고자 하는 데이터셋을 업로드합니다.')
        tabs = st.tabs(["Upload Data", "From Git", "From Kaggle Samples"])
        with tabs[0]:
            self.view_metadata()
            # 고유한 키를 부여한 파일 업로더 위젯 (UUID를 사용하여 고유한 키 생성)
            # analyze_button = st.button("Upload Data", key=f"data_analyze_button")
            training_uploaded_files = st.file_uploader("Choose training data zip file", type=["zip"], key=f"training_data_analyze_file_uploader")
            inference_uploaded_files = st.file_uploader("Choose inferencing data zip file", type=["zip"], key=f"inference_data_analyze_file_uploader")
            if training_uploaded_files:
                # training 파일을 data/train 폴더에 저장하고 압축해제
                self.save_and_extract_zip(training_uploaded_files, self.train_data_path)
            if inference_uploaded_files:
                # inferencing 파일을 data/inference 폴더에 저장하고 압축해제
                self.save_and_extract_zip(inference_uploaded_files, self.inference_data_path)
        with tabs[1]:
            self.view_metadata()
            git_url = st.text_input("Git Repository URL")
            train_file_path = st.text_input("Path to train data file  (must be *.tar.gz or *.zip)")
            infer_file_path = st.text_input("Path to inference data file  (must be *.tar.gz or *.zip)")
            if st.button("Download from Git"):
                if git_url and train_file_path:
                    download_train_path = self.download_from_git(git_url, train_file_path)
                    self.extract_zip(download_train_path, self.train_data_path)
                if git_url and infer_file_path:
                    download_inference_path = self.download_from_git(git_url, infer_file_path)
                    self.extract_zip(download_inference_path, self.inference_data_path)
                if git_url and (train_file_path or infer_file_path):
                    st.success("Files downloaded successfully!")
                else:
                    st.error("Please provide Git URL and both File Paths.")
        with tabs[2]:
            self.view_metadata()
            resp = ""
            rec_datasets = [""]
            kaggle_api_txt = {}
            if ('rec_datasets' in st.session_state) and ('resp' in st.session_state):
                resp  = st.session_state['resp'] 
                rec_datasets  = st.session_state['rec_datasets'] 
            if st.button("Recommend dataset"):
                st.write("Recommend Kaggle dataset samples according to your source code.")
                kaggle_data_recommender = KaggleDataRecommender(self.api, self.metadata_path) 
                resp, rec_datasets, kaggle_api_txt = kaggle_data_recommender.recommend_kaggle_data() 
                st.session_state['kaggle_api_txt'] = kaggle_api_txt
                st.session_state['resp'] = resp 
                st.session_state['rec_datasets'] = rec_datasets 
            option = st.selectbox(
                resp,
                tuple(rec_datasets),
                # label_visibility=st.session_state.visibility,
                # disabled=st.session_state.disabled,
            )
            if st.button("Download from Kaggle"):
                if option: 
                    print(f'{option} selected.')
                    meta_data_info = ''
                    kaggle_api_txt = st.session_state['kaggle_api_txt']
                    for dataset_name in kaggle_api_txt.keys():
                        metadata = kaggle_api_txt[dataset_name]
                        if metadata["id"] == option:
                            print(metadata["description"])
                            meta_data_info = metadata["description"]
                            with open('data_info', 'w', encoding='utf-8') as file:
                                # 문자열을 파일에 쓰기
                                file.write(meta_data_info)
                    try: 
                        #download_train_path = self.download_from_kaggle(option)
                        temp_dir = f"/tmp/{uuid4()}/"
                        os.makedirs(temp_dir, exist_ok=True)
                        _ = subprocess.run(['kaggle', 'datasets', 'download', '-d', option, '-p', temp_dir])
                        self.extract_zip(temp_dir + option.split('/')[-1] + '.zip', self.train_data_path)
                        if os.path.exists(self.inference_data_path):
                            shutil.rmtree(self.inference_data_path)
                        # copy train data to inference dir 
                        shutil.copytree(self.train_data_path, self.inference_data_path)
                        st.success("Kaggle data downloaded successfully!")
                    except Exception as e: 
                        st.error(f"Failed downloading data from Kaggle {e}")
                else: 
                    st.warning('None of Kaggle dataset selected.')

        
    def download_from_git(self, repo_url, file_path):
        save_folder = self.data_path
        valid_formats = [".tar.gz", ".zip"]
        if not any(file_path.endswith(fmt) for fmt in valid_formats):
            st.error("Only *.tar.gz or *.zip formats are supported.")
            return
        temp_dir = f"/tmp/{uuid4()}"
        os.makedirs(temp_dir, exist_ok=True)
        try:
            Repo.clone_from(repo_url, temp_dir)
            src_file = os.path.join(temp_dir, file_path)
            dest_file = os.path.join(save_folder, os.path.basename(src_file))
            os.makedirs(save_folder, exist_ok=True)
            if os.path.exists(src_file):
                with open(dest_file, "wb") as f_dest:
                    with open(src_file, "rb") as f_src:
                        f_dest.write(f_src.read())
            else:
                st.error(f"{file_path} does not exist in the repo.")
        except Exception as e:
            st.error(f"Error: {e}")
        finally:
            shutil.rmtree(temp_dir)
            return dest_file
    
    def _get_data_hierarchy(self, path):
        data_hierarchy = {}
        for root, dirs, files in os.walk(path):
            structure = data_hierarchy
            parts = root.replace(path, '').strip(os.sep).split(os.sep)
            for part in parts:
                if part:
                    if part not in structure:
                        structure[part] = {}
                    structure = structure[part]
            for d in dirs:
                structure[d] = {}
            for f in files:
                structure[f] = os.path.join(root, f)

        return data_hierarchy

    def write_metadata(self):
        train_path = os.path.join(self.data_path, 'train')
        inference_path = os.path.join(self.data_path, 'inference')
        if not os.path.exists(DATA_META_FILE):
            if not os.path.exists(DATA_META_PATH):
                 os.makedirs(DATA_META_PATH)
            null_json = {}
            with open(DATA_META_FILE , 'w') as file:
                json.dump(null_json, file)  # null로 초기화
        try: 
            with open('data_info', 'r', encoding='utf-8') as file:
                # 파일 내용 읽기
                data_description = file.read() + "If you have any additional information, please use it."
                y_description = "If you have any additional information, please use it."
            if os.path.exists('data_info'):
                os.remove('data_info')
        except Exception as e:
            read_content = ""
            with open(DATA_META_FILE, 'r', encoding='utf-8') as file:
                data = json.load(file)  # json.load를 사용하여 파일 내용을 파싱
            y_description = data.get("y_description:", {}).get("y_description", "")
            data_description = data.get("data_description:", {}).get("data_description", "")
            if y_description =="":
                y_description = "If you have any additional information, please use it."
            if data_description == "":
                data_description = "If you have any additional information, please use it."
        train_hierarchy = self._get_data_hierarchy(train_path)
        inference_hierarchy = self._get_data_hierarchy(inference_path)
        metadata = {
            "data_path": {
                "train": train_path,
                "inference": inference_path
            },
            "data_hierarchy": {
                "train": train_hierarchy,
                "inference": inference_hierarchy
            },
            "y_description:": {'y_description' : y_description},
            "data_description:": {'data_description' : data_description}
        }

        data_meta = {
        'data_meta': metadata
        }
        if not os.path.exists(DATA_META_PATH):
            os.makedirs(DATA_META_PATH)
        with open(DATA_META_PATH + "data_meta.json", 'w', encoding='utf-8') as json_file:
            json.dump(metadata, json_file, ensure_ascii=False, indent=4)
        return data_meta

    def render(self):
        self.set_input()
        self.write_metadata()
          
  